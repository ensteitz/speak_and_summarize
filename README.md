# speak_and_summarize
Comparative study of prompt engineering strategies (zero-shot, role-based, and chain-of-thought) applied to Whisper-transcribed TED Talks. Summaries are generated using GPT-3.5-turbo and T5-small, evaluated with ROUGE and BERTScore. Built for LING-L 715 (Spring 2025).

# ğŸ—£ï¸ Speak & Summarize
**A Comparative Study of Prompt Engineering for Whisper-Transcribed Lectures**  
_By Rin Steitz | LING-L 715 Final Project | Spring 2025_

---

## ğŸ“Œ Project Overview

This project investigates how prompt engineering impacts the quality of summaries generated by large language models (LLMs) from spoken content. I compare three prompting strategiesâ€”**zero-shot**, **role-based**, and **chain-of-thought**â€”on TED Talk transcripts generated using OpenAI's **Whisper-large-v3**.

Two summarization models were used:
- **GPT-3.5-turbo** (via OpenAI API)
- **T5-small** (via Hugging Face Transformers)

Summaries are automatically evaluated using **ROUGE-1/2/L** and **BERTScore (Precision, Recall, Fâ‚)**.

---

## ğŸ§ª Key Findings

- Role-based prompting produced the **best performance** for GPT-3.5-turbo (ROUGE-1 = 0.383, BERTScore Fâ‚ = 0.209).
- T5-small performed best under chain-of-thought prompts (ROUGE-1 = 0.248, BERTScore Fâ‚ = 0.074).
- Most common errors were **omissions** (especially numerical detail), followed by occasional **hallucinations** in smaller models.

---

## ğŸ—ƒï¸ Project Structure

```bash
â”œâ”€â”€ audio_files/               # Original TED Talk MP3s
â”œâ”€â”€ transcripts/               # Whisper-transcribed and cleaned transcripts
â”œâ”€â”€ reference_summaries/       # Manually written reference summaries
â”œâ”€â”€ prompts/                   # Prompt templates used for each experiment
â”œâ”€â”€ summaries_openai/          # GPT-3.5-turbo generated summaries
â”œâ”€â”€ summaries_t5_local/        # T5-small generated summaries
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate_all_rouge.py
â”‚   â”œâ”€â”€ evaluate_bertscore.py
â”‚   â”œâ”€â”€ rouge_results.csv
â”‚   â”œâ”€â”€ bertscore_results.csv
â”‚   â””â”€â”€ metric_correlation.png
â”œâ”€â”€ visuals/                   # Figures from the paper (bar plots, tables, etc.)
â”œâ”€â”€ transcribe.py              # Whisper transcription script
â”œâ”€â”€ gpt3.5_turbo_summarize.py  # GPT-3.5 summarization pipeline
â”œâ”€â”€ t5_local_summarize.py      # T5-small summarization pipeline
â””â”€â”€ LING715_final_report.pdf   # Final written report (LaTeX compiled PDF)
